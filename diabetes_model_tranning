{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91723,"databundleVersionId":14272474,"sourceType":"competition"}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# 1. IMPORTS & SETUP\n# =========================\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"âœ… Imports completed\")\n\n\n# =========================\n# 2. LOAD DATA\n# =========================\ntrain_df = pd.read_csv(\"/kaggle/input/playground-series-s5e12/train.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/playground-series-s5e12/test.csv\")\n\nprint(\"âœ… Data loaded\")\nprint(\"Train shape:\", train_df.shape)\nprint(\"Test shape :\", test_df.shape)\n\n\n# =========================\n# 3. BASIC CLEANING\n# =========================\nTARGET = \"diagnosed_diabetes\"\nID_COL = \"id\"\n\nX = train_df.drop(columns=[TARGET, ID_COL])\ny = train_df[TARGET]\nX_test = test_df.drop(columns=[ID_COL])\n\nprint(\"âœ… Feature/Target separated\")\n\n\n# =========================\n# 4. ENCODE CATEGORICAL FEATURES\n# =========================\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n\nencoders = {}\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col].astype(str))\n    X_test[col] = le.transform(X_test[col].astype(str))\n    encoders[col] = le\n\nprint(f\"âœ… Encoded {len(categorical_cols)} categorical columns\")\n\n\n# =========================\n# 5. ADVANCED LIGHTGBM PARAMETERS\n# =========================\nlgb_params = {\n    \"objective\": \"binary\",\n    \"metric\": \"auc\",\n    \"boosting_type\": \"gbdt\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 128,\n    \"max_depth\": 12,\n    \"min_child_samples\": 30,\n    \"subsample\": 0.8,\n    \"subsample_freq\": 1,\n    \"colsample_bytree\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"n_estimators\": 5000,\n    \"random_state\": 42,\n    \"n_jobs\": -1\n}\n\nprint(\"âœ… LightGBM parameters set\")\n\n\n# =========================\n# 6. STRATIFIED K-FOLD CV\n# =========================\nN_FOLDS = 5\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n\noof_preds = np.zeros(len(X))\ntest_preds = np.zeros(len(X_test))\n\nprint(\"ðŸš€ Starting Stratified K-Fold Training...\\n\")\n\n\n# =========================\n# 7. TRAINING LOOP\n# =========================\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    print(f\"ðŸ”¹ Fold {fold+1}/{N_FOLDS} started\")\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgb.LGBMClassifier(**lgb_params)\n    \n    model.fit(\n        X_train,\n        y_train,\n        eval_set=[(X_val, y_val)],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=200),\n            lgb.log_evaluation(period=200)\n        ]\n    )\n    \n    val_pred = model.predict_proba(X_val)[:, 1]\n    oof_preds[val_idx] = val_pred\n    \n    fold_auc = roc_auc_score(y_val, val_pred)\n    print(f\"âœ… Fold {fold+1} ROC AUC: {fold_auc:.6f}\\n\")\n    \n    test_preds += model.predict_proba(X_test)[:, 1] / N_FOLDS\n\n\n# =========================\n# 8. OVERALL CV SCORE\n# =========================\noverall_auc = roc_auc_score(y, oof_preds)\nprint(\"ðŸ”¥ FINAL OOF ROC AUC:\", round(overall_auc, 6))\n\n\n# =========================\n# 9. CREATE SUBMISSION FILE\n# =========================\nsubmission = pd.DataFrame({\n    \"id\": test_df[ID_COL],\n    \"diagnosed_diabetes\": test_preds\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"âœ… submission.csv generated successfully\")\nprint(submission.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-21T04:05:42.301126Z","iopub.execute_input":"2025-12-21T04:05:42.301409Z","iopub.status.idle":"2025-12-21T04:15:05.009629Z","shell.execute_reply.started":"2025-12-21T04:05:42.301386Z","shell.execute_reply":"2025-12-21T04:15:05.008780Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sqlalchemy/orm/query.py:195: SyntaxWarning: \"is not\" with 'tuple' literal. Did you mean \"!=\"?\n  if entities is not ():\n","output_type":"stream"},{"name":"stdout","text":"âœ… Imports completed\nâœ… Data loaded\nTrain shape: (700000, 26)\nTest shape : (300000, 25)\nâœ… Feature/Target separated\nâœ… Encoded 6 categorical columns\nâœ… LightGBM parameters set\nðŸš€ Starting Stratified K-Fold Training...\n\nðŸ”¹ Fold 1/5 started\n[LightGBM] [Info] Number of positive: 349045, number of negative: 210955\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031395 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1640\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623295 -> initscore=0.503556\n[LightGBM] [Info] Start training from score 0.503556\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's auc: 0.722754\n[400]\tvalid_0's auc: 0.725962\n[600]\tvalid_0's auc: 0.726944\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[800]\tvalid_0's auc: 0.727364\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1000]\tvalid_0's auc: 0.727625\n[1200]\tvalid_0's auc: 0.727737\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1400]\tvalid_0's auc: 0.72768\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nEarly stopping, best iteration is:\n[1255]\tvalid_0's auc: 0.727755\nâœ… Fold 1 ROC AUC: 0.727755\n\nðŸ”¹ Fold 2/5 started\n[LightGBM] [Info] Number of positive: 349045, number of negative: 210955\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033462 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1641\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623295 -> initscore=0.503556\n[LightGBM] [Info] Start training from score 0.503556\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's auc: 0.720684\n[400]\tvalid_0's auc: 0.72389\n[600]\tvalid_0's auc: 0.724995\n[800]\tvalid_0's auc: 0.72554\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1000]\tvalid_0's auc: 0.725828\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1200]\tvalid_0's auc: 0.726075\n[1400]\tvalid_0's auc: 0.726189\n[1600]\tvalid_0's auc: 0.72628\n[1800]\tvalid_0's auc: 0.726353\n[2000]\tvalid_0's auc: 0.726384\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[2200]\tvalid_0's auc: 0.726393\nEarly stopping, best iteration is:\n[2093]\tvalid_0's auc: 0.72642\nâœ… Fold 2 ROC AUC: 0.726420\n\nðŸ”¹ Fold 3/5 started\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034382 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1643\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's auc: 0.721163\n[400]\tvalid_0's auc: 0.724876\n[600]\tvalid_0's auc: 0.725955\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[800]\tvalid_0's auc: 0.726524\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1000]\tvalid_0's auc: 0.726845\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1200]\tvalid_0's auc: 0.726985\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1400]\tvalid_0's auc: 0.727125\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1600]\tvalid_0's auc: 0.727233\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1800]\tvalid_0's auc: 0.727129\nEarly stopping, best iteration is:\n[1623]\tvalid_0's auc: 0.727273\nâœ… Fold 3 ROC AUC: 0.727273\n\nðŸ”¹ Fold 4/5 started\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031545 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1640\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's auc: 0.722138\n[400]\tvalid_0's auc: 0.725739\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[600]\tvalid_0's auc: 0.726937\n[800]\tvalid_0's auc: 0.727515\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1000]\tvalid_0's auc: 0.727794\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1200]\tvalid_0's auc: 0.727883\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1400]\tvalid_0's auc: 0.727981\nEarly stopping, best iteration is:\n[1360]\tvalid_0's auc: 0.728007\nâœ… Fold 4 ROC AUC: 0.728007\n\nðŸ”¹ Fold 5/5 started\n[LightGBM] [Info] Number of positive: 349046, number of negative: 210954\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031917 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1639\n[LightGBM] [Info] Number of data points in the train set: 560000, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.623296 -> initscore=0.503564\n[LightGBM] [Info] Start training from score 0.503564\nTraining until validation scores don't improve for 200 rounds\n[200]\tvalid_0's auc: 0.722326\n[400]\tvalid_0's auc: 0.725844\n[600]\tvalid_0's auc: 0.726757\n[800]\tvalid_0's auc: 0.727159\n[1000]\tvalid_0's auc: 0.727426\n[1200]\tvalid_0's auc: 0.727622\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1400]\tvalid_0's auc: 0.727785\n[1600]\tvalid_0's auc: 0.727791\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[1800]\tvalid_0's auc: 0.727821\n[2000]\tvalid_0's auc: 0.72779\nEarly stopping, best iteration is:\n[1861]\tvalid_0's auc: 0.727882\nâœ… Fold 5 ROC AUC: 0.727882\n\nðŸ”¥ FINAL OOF ROC AUC: 0.727458\nâœ… submission.csv generated successfully\n       id  diagnosed_diabetes\n0  700000            0.496250\n1  700001            0.706853\n2  700002            0.776797\n3  700003            0.365917\n4  700004            0.927125\n","output_type":"stream"}],"execution_count":1}]}